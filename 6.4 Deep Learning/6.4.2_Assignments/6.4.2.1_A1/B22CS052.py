# -*- coding: utf-8 -*-
"""B22CS052.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kp2GTIy949sZP-ZaGGrSk7c_-FU9Nzvf

# Deep Learning CSL7590 Assignment 01:

## Building Neural Network from scratch

### Objective:
In this assignment, you are required to implement a neural network from scratch in Python.
Build a feedforward neural network and implement backpropagation for training. By the end of
this assignment, you should have a working neural network that can be trained on a simple
dataset for multi-class classification.
"""

# All essential libraries needed
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

#Dataset library
from torchvision.datasets import MNIST
from torchvision.transforms import transforms
from sklearn.model_selection import train_test_split
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.metrics import confusion_matrix

"""### Part 01: Dataset

#### Dataset is being loaded from pyTorch
"""

# Loading the dataset and applying the necessay transformations
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,),(0.3081,))
])

train_data = MNIST(root="./data",train=True,download = True,transform=transform)

# reshaping the dataset and standarizing it
data_set = train_data.data
data_set = data_set.reshape(60000,784)
data_set = data_set/255
print(data_set.shape)

# Analysis of data
print(f"Number of images: {train_data.data.shape}")

rand_idxs = [np.random.randint(0,60000) for i in range(5)]

fig, axes = plt.subplots(1,5,figsize=(16,9),gridspec_kw={'hspace': 0.5})

for i,rand_idx in enumerate(rand_idxs):
  axes[i].imshow(train_data.data[rand_idx],cmap="gray")
  axes[i].axis("off")

"""### Part 02: Class Neural Network"""

class NeuralNetwork:

  def __init__(self,input_size,output_size,hidden_layer_size_1,hidden_layer_size_2,activation_function,learning_rate):
    """
      input_size : The number of input neurons needed (dimension of data given). For MNIST it is 784 (28x28)
      output_size: The number of output layer neurons needed (dimension of output). For MNIST it is 10 (0-9)
      hidden_layer_size_i: Specifies the number of neurons in hidden layer i
      activation_function: The activation function to be used while training
    """
    self.input_size = input_size
    self.output_size = output_size
    self.hidden_layer_1 = hidden_layer_size_1
    self.hidden_layer_2 = hidden_layer_size_2
    self.learning_rate = learning_rate

    # supporting three activation functions
    if activation_function == "tanh":
      self.sigmoid = False
      self.relu = False
      self.tanh = True
    elif activation_function == "relu":
      self.sigmoid = False
      self.relu = True
      self.tanh = False
    else:
      self.sigmoid = True
      self.relu = False
      self.softmax = False

    self.batch_size = 22
    np.random.seed(52)

    #initializing weights and biases
    self.w1 = np.random.randn(self.input_size,self.hidden_layer_1)
    self.w2 = np.random.randn(self.hidden_layer_1,self.hidden_layer_2)
    self.w3 = np.random.randn(self.hidden_layer_2,output_size)
    self.b1 = np.ones(self.hidden_layer_1)
    self.b2 = np.ones(self.hidden_layer_2)
    self.b3 = np.ones(self.output_size)

  def train(self,train_data,y):

    # keeping track of epoch losses and accuracies
    self.epoch_losses = []
    self.epoch_accuracies = []

    for epoch in range(25):

      # keeping track of batch_loss per epoch
      batch_losses = []
      # I am using serial batch distribution later, so shuffling here itself
      indices = np.arange(train_data.shape[0])
      np.random.shuffle(indices)
      data = train_data[indices]
      labels = y[indices]

      # storing the tuple (batch_data,batch_labels)
      batches = []
      for i in range(0,train_data.shape[0], self.batch_size):
        batch_data = data[i:i+self.batch_size]
        batch_labels = labels[i:i+self.batch_size]
        batches.append((batch_data,batch_labels))

      # training for a batch
      i = 1
      for batch_data,batch_labels in batches:
        i+=1
        batch_loss = self.feed_forward(batch_data,batch_labels)
        batch_losses.append(batch_loss)

        self.back_propagation(batch_labels,batch_data)

      # take epoch loss,accuracy, append it to the necessary lists
      loss,accuracy,_ = self.test(data,labels)
      self.epoch_accuracies.append(accuracy)

      print(f"Epoch {epoch+1} Loss {np.average(np.array(batch_losses))}, Epoch Accuracy: {accuracy}")

      self.epoch_losses.append(np.average(np.array(batch_losses)))

    self.plot_epoch_stats()


  def plot_epoch_stats(self):
    plt.figure(figsize=(10, 4))
    plt.subplot(1, 2, 1)
    plt.plot(self.epoch_losses, label="Train Loss")
    plt.title("Loss per Epoch")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(self.epoch_accuracies, label="Train Accuracy")
    plt.title("Accuracy per Epoch")
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy")
    plt.legend()

    plt.tight_layout()
    plt.show()


  def feed_forward(self,data,y):

    #Standard Feed forward referenced from slides

    self.pre_activation_1 = np.dot(data,self.w1) + self.b1
    # print("1 pre",self.pre_activation_1[0],self.pre_activation_1[1])
    self.activation_1 = self.activation(self.pre_activation_1)
    # print("1 act",self.activation_1[0],self.activation_1[1])
    self.pre_activation_2 = np.dot(self.activation_1,self.w2) + self.b2
    # print("2 pre",self.pre_activation_2[0],self.pre_activation_2[1])
    self.activation_2 = self.activation(self.pre_activation_2)
    # print("2 act",self.activation_2[0],self.activation_2[1])
    self.pre_activation_3 = np.dot(self.activation_2,self.w3) + self.b3
    # print("3 pre",self.pre_activation_3[0],self.pre_activation_3[1])
    self.y_hat = self.softmax_apply(self.pre_activation_3)
    # print("Y hat",self.y_hat[0],self.y_hat[1])

    loss_in_batch = self.calculate_loss(self.y_hat,y)

    return loss_in_batch


  def activation(self,z):

    #Again standard activation functions
    if self.sigmoid:
      return 1.0/(1.0 + np.exp(-z))
    elif self.relu:
      return np.maximum(0, z)
    elif self.tanh:
      return np.tanh(z)


  def back_propagation(self,y,batch):

    # Implemented based on algorithm seen in slides posted on google classroom

    dL_da3 = self.y_hat - y
    batch_size = batch.shape[0]
    dL_dw3 = np.dot(self.activation_2.T , dL_da3) / batch_size
    dL_db3 = np.average(dL_da3,keepdims =True)

    dL_dh2 = np.dot(dL_da3, self.w3.T)
    dL_da2 = dL_dh2 * self.activation_derivative(self.pre_activation_2)
    dL_dw2 = np.dot(self.activation_1.T , dL_da2) / batch_size
    dL_db2 = np.average(dL_da2,keepdims =True)

    dL_dh1 = np.dot(dL_da2, self.w2.T)
    dL_da1 = dL_dh1 * self.activation_derivative(self.pre_activation_1)
    dL_dw1 = np.dot(batch.T , dL_da1) / batch_size
    dL_db1 = np.average(dL_da1,keepdims =True)

    self.w3 -= self.learning_rate * dL_dw3
    self.b3 -= self.learning_rate * dL_db3.flatten()
    self.w2 -= self.learning_rate * dL_dw2
    self.b2 -= self.learning_rate * dL_db2.flatten()
    self.w1 -= self.learning_rate * dL_dw1
    self.b1 -= self.learning_rate * dL_db1.flatten()


  def test(self,x,y):

    pre_activation_1 = np.dot(x,self.w1) + self.b1
    # print("1 pre",self.pre_activation_1[0],self.pre_activation_1[1])
    activation_1 = self.activation(pre_activation_1)
    # print("1 act",self.activation_1[0],self.activation_1[1])
    pre_activation_2 = np.dot(activation_1,self.w2) + self.b2
    # print("2 pre",self.pre_activation_2[0],self.pre_activation_2[1])
    activation_2 = self.activation(pre_activation_2)
    # print("2 act",self.activation_2[0],self.activation_2[1])
    pre_activation_3 = np.dot(activation_2,self.w3) + self.b3
    # print("3 pre",self.pre_activation_3[0],self.pre_activation_3[1])
    y_hat = self.softmax_apply(pre_activation_3)
    # print("Y hat",self.y_hat[0],self.y_hat[1])
    loss = self.calculate_loss(y_hat,y)
    count = 0
    for i,prediction in enumerate(y_hat):
      if (np.argmax(prediction) == np.argmax(y[i])):
        count+=1

    return loss,count/y.shape[0],y_hat


  def softmax_apply(self, z):
    # Separately created to handle the output function, for the last layer
    exp_values = np.exp(z - np.max(z, axis=1, keepdims=True))
    return exp_values / np.sum(exp_values, axis=1, keepdims=True)

  def activation_derivative(self,x):
    # returns the derivative of activation functions
    if self.sigmoid:
      a = self.activation(x)
      return a*(1-a)
    elif self.relu:
      return np.where(x <= 0,0,1)
    elif self.tanh:
      a = self.activation(x)
      return 1 - np.square(a)

  def calculate_loss(self, y_hat, y):
    # considered cross entropy loss
    return -np.sum(y * np.log(y_hat + 1e-8)) / y.shape[0]

"""### Part 03: Testing The Neural Network for two activation functions: ReLU and Tahn and for three splits given: 0.7, 0.8 and 0.9 ratio"""

def one_hot_encode(y_original):
  y = []
  for ele in y_original:
    one_hot = [int(i == ele) for i in range(0,10)]
    y.append(one_hot)

  return np.array(y)



train_split = [0.7,0.8,0.9]
data_set_labels = one_hot_encode(train_data.targets)
for split in train_split:
  train_data_set,test_data_set,train_labels,test_labels = train_test_split(data_set,data_set_labels,train_size=split,random_state=42)
  nn = NeuralNetwork(784,10,128,64,"relu",0.01)
  nn.train(train_data_set,train_labels)
  test_loss,test_accuracy,predictions = nn.test(test_data_set,test_labels)
  print(f"Test Loss for Train Ratio {split} is: {test_loss} and Test Accuracy is: {test_accuracy}")


  true_labels = np.argmax(test_labels, axis=1)
  cm = confusion_matrix(true_labels, np.argmax(predictions,axis=1))
  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.arange(10))
  disp.plot(cmap=plt.cm.Blues, values_format='d')

  plt.title("Confusion Matrix")
  plt.show()

  nn = NeuralNetwork(784,10,128,64,"tanh",0.01)
  nn.train(train_data_set,train_labels)
  test_loss,test_accuracy,predicted = nn.test(test_data_set,test_labels)
  print(f"Test Loss for Train Percent {split} is: {test_loss} and Test Accuracy is: {test_accuracy}")

  true_labels = np.argmax(test_labels, axis=1)
  cm = confusion_matrix(true_labels, np.argmax(predictions,axis=1))
  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.arange(10))
  disp.plot(cmap=plt.cm.Blues, values_format='d')

  plt.title("Confusion Matrix")
  plt.show()